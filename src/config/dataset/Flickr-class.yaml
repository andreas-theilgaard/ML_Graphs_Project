dataset_name: 'Flickr'
task: 'NodeClassification'
metrics: ['acc','f1-micro','f1-macro']
track_metric: 'acc'

DownStream:
  saved_embeddings: False
  using_features: True
  use_spectral: False
  random: False
  K: 8
  training:
    epochs: 300
    num_layers: 2
    dropout: 0.5
    lr: 0.01
    hidden_channels: 64
    batchnorm: False
    weight_decay: 0.0


Node2Vec:
  training:
    embedding_dim: 64
    walk_length: 20
    context_size: 10
    walks_per_node: 10
    epochs: 10
    lr: 0.01
    batch_size: 65536
    num_negative_samples: 1
    sparse: True
    num_workers: 4

GNN:
  model: 'GCN'
  extra_info: False
  training:
    hidden_channels: 64
    num_layers: 2
    dropout: 0.5
    lr: 0.001
    epochs: 200
    batchnorm: False
    weight_decay: 0.0

Shallow:
  training:
    lr: 0.01
    epochs: 200
    init_beta: 0.0
    embedding_dim: 64
    init: 'laplacian'
    decode_type: 'dist'
    train_batch: True
    batch_size: 65536
    weight_decay: 0.000001


combined:
  type: 'comb2'
  comb1:
    training:
      shallow_lr: 0.01
      init_beta: 0.0
      embedding_dim: 64
      init: 'laplacian'
      decode_type: 'dist'

      deep_model: 'GCN'
      deep_hidden_channels: 64
      deep_lr: 0.001
      deep_out_dim: 64
      deep_num_layers: 2
      deep_dropout: 0.0
      deep_decode: 'dist'
      gamma: 0.0

      MLP_HIDDEN: 64
      MLP_NUM_LAYERS: 2
      MLP_DROPOUT: 0.5
      MLP_LR: 0.01
      MLP_EPOCHS: 300
      APPLY_BATCHNORM: False


      balance: False
      warm_start: 200
      joint_train: 200
      shallow_lr_joint: 0.01
      deep_lr_joint: 0.001
      lambda_lr: 0.01
      lambda_: 1.0
      batch_size: 65536
      direction: 'deep_first'
      shallow_frozen_epochs: 50
      deep_frozen_epochs: 50

  comb2:
    training:
      shallow_lr: 0.01
      init_beta: 0.0
      embedding_dim: 64
      init: 'laplacian'
      decode_type: 'dist'
      SHALLOW_WARM_START: 200
      SHALLOW_TRAIN_BATCH: True
      SHALLOW_WARM_BATCH_SIZE: 65536


      deep_model: 'GCN'
      deep_hidden_channels: 64
      deep_lr: 0.001
      deep_out_dim: 64
      deep_num_layers: 2
      deep_dropout: 0.5

      MLP_HIDDEN: 64
      MLP_NUM_LAYERS: 2
      MLP_DROPOUT: 0.5
      MLP_LR: 0.01
      APPLY_BATCHNORM: False

      epochs: 200
      batch_size: 65536
      direction: 'deep_first'
      shallow_frozen_epochs: 100
      deep_frozen_epochs: 200
