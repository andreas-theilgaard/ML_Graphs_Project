dataset_name: 'ogbn-arxiv'
task: 'NodeClassification'
metrics: ['acc','f1-micro','f1-macro']
track_metric: 'acc'

DownStream:
  saved_embeddings: False
  using_features: True
  use_spectral: False
  random: False
  K: 16
  training:
    epochs: 100
    num_layers: 3
    dropout: 0.5
    lr: 0.01
    hidden_channels: 256
    batchnorm: True
    weight_decay: 0

Node2Vec:
  training:
    embedding_dim: 128
    walk_length: 80
    context_size: 20
    walks_per_node: 10
    epochs: 5
    lr: 0.01
    batch_size: 256
    num_negative_samples: 1
    sparse: True
    num_workers: 4

GNN:
  model: 'GraphSage'
  extra_info: False
  training:
    hidden_channels: 256
    num_layers: 3
    dropout: 0.5
    lr: 0.01
    epochs: 400
    use_valedges: False
    batchnorm: True
    weight_decay: 0

GNN_DIRECT:
  model: 'GraphSage'
  extra_info: False
  training:
    hidden_channels: 256
    num_layers: 3
    dropout: 0.0
    lr: 0.01
    epochs: 50
    weight_decay: 0
    decode_type: 'dot'
    init_beta: 0.0
    train_batch: False
    batch_size: 65536



Shallow:
  training:
    lr: 0.1
    epochs: 50
    init_beta: 0.0
    embedding_dim: 2
    init: 'laplacian'
    decode_type: 'dist'
    train_batch: False
    batch_size: 65536
    weight_decay: 0


combined:
  type: 'comb1'
  comb1:
    training:
      shallow_lr: 0.1
      init_beta: 0.0
      embedding_dim: 64
      init: 'laplacian'
      decode_type: 'dist'

      deep_model: 'GraphSage'
      deep_hidden_channels: 256
      deep_lr: 0.01
      deep_out_dim: 64
      deep_num_layers: 3
      deep_dropout: 0.0
      deep_decode: 'dist'
      gamma: 0.0

      MLP_HIDDEN: 128
      MLP_NUM_LAYERS: 3
      MLP_DROPOUT: 0.5
      MLP_LR: 0.01
      MLP_EPOCHS: 500
      APPLY_BATCHNORM: True



      balance: False
      warm_start: 100
      joint_train: 300
      shallow_lr_joint: 0.1
      deep_lr_joint: 0.01
      lambda_lr: 0.01
      lambda_: 1.0
      batch_size: 65536
      direction: 'shallow_first'
      shallow_frozen_epochs: 0
      deep_frozen_epochs: 50

  comb2:
    training:
      shallow_lr: 0.1
      init_beta: 0.0
      embedding_dim: 64
      init: 'laplacian'
      decode_type: 'dist'
      SHALLOW_WARM_START: 30
      SHALLOW_TRAIN_BATCH: True
      SHALLOW_WARM_BATCH_SIZE: 65536


      deep_model: 'GraphSage'
      deep_hidden_channels: 256
      deep_lr: 0.01
      deep_out_dim: 256
      deep_num_layers: 3
      deep_dropout: 0.5

      MLP_HIDDEN: 256
      MLP_NUM_LAYERS: 2
      MLP_DROPOUT: 0.0
      MLP_LR: 0.01
      APPLY_BATCHNORM: True

      epochs: 400
      batch_size: 65536
      direction: 'deep_first'
      shallow_frozen_epochs: 100
      deep_frozen_epochs: 100
